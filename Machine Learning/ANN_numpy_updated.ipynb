{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0         1         2         3         4         5         6   \\\n",
      "0     -0.446891 -0.013397  0.232645  2.156649  1.652923 -0.210531 -0.662227   \n",
      "1      0.243747 -1.144175 -0.622214 -0.661979 -0.373315  0.520313 -1.307954   \n",
      "2     -1.417799 -0.088833 -0.647181 -1.141497 -1.321100  0.686798 -1.045569   \n",
      "3      0.656784  0.381201 -1.039011  1.285315  0.808324  1.428519 -1.144367   \n",
      "4      1.098520  1.706769 -1.030370  2.001009  2.751551  1.202229 -0.941066   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "19995  0.146324 -0.618555  0.547413  0.851301  0.838880 -1.248402  0.698199   \n",
      "19996 -2.227923 -0.327196  1.258849 -0.945254  3.925262  0.316686 -1.061159   \n",
      "19997 -0.800405  1.477997  0.090057 -0.973997 -0.204970  0.224539  0.302265   \n",
      "19998 -1.217344 -0.785401 -0.413781 -0.335324  0.181671 -1.685733 -0.437981   \n",
      "19999  1.601005 -0.890398 -1.425049  0.887501  0.167205 -0.395042 -0.512554   \n",
      "\n",
      "             7         8         9         10        11        12        13  \\\n",
      "0      0.705144  1.434684 -0.445750 -0.178501 -0.905503 -0.594099 -0.728586   \n",
      "1      1.639481 -2.838816  0.331636  0.100261 -1.173067 -0.079774  2.194899   \n",
      "2     -0.376153 -2.272447 -0.669733 -1.070732  1.091993  0.692293  0.937617   \n",
      "3     -0.609958  0.983676  0.060917  0.471647 -0.594279  2.104027 -1.113597   \n",
      "4      1.965282  2.979131  1.402483 -1.432987  0.400322 -0.839851 -1.580974   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "19995 -0.545033  0.994348 -0.054620  0.581239 -1.842074 -0.552879 -0.102096   \n",
      "19996  1.846706  3.692157  0.698035 -0.184239 -0.546036 -0.457919 -1.030581   \n",
      "19997  0.361146 -1.906636  1.381875  0.034366  1.690199 -0.901569  2.183842   \n",
      "19998 -1.176626 -1.434436 -0.880986  2.117703 -2.136842 -1.185261  1.547704   \n",
      "19999  0.050777 -0.034945 -0.289219  0.639189  0.981751 -0.110296 -0.187214   \n",
      "\n",
      "             14        15        16        17        18        19  \n",
      "0     -0.627002 -0.240520  0.131696 -0.945878  1.840922 -0.279024  \n",
      "1      0.242839  0.412164 -0.328714  0.902931 -1.418728 -0.290139  \n",
      "2      0.262142  0.399238  0.614614  1.209465 -1.526386  0.337771  \n",
      "3     -0.188799  1.226253 -1.067763 -0.350275 -0.746426 -1.258696  \n",
      "4     -0.659587  2.292688  0.494188 -1.583187 -2.468254 -0.469648  \n",
      "...         ...       ...       ...       ...       ...       ...  \n",
      "19995  0.307538 -1.267832 -0.165875 -0.717397 -0.808588  0.796346  \n",
      "19996  2.406014 -0.978943 -0.086879 -1.964453 -0.875574  1.535930  \n",
      "19997  0.831008  0.036308  0.516088  0.378670  0.110968 -0.176363  \n",
      "19998  1.174712 -0.429117  0.132068  0.249277 -0.941496  1.661661  \n",
      "19999 -0.226811 -0.547772 -0.006706 -0.023773  1.220809  1.489330  \n",
      "\n",
      "[20000 rows x 20 columns]\n",
      "       0\n",
      "0      3\n",
      "1      2\n",
      "2      2\n",
      "3      3\n",
      "4      3\n",
      "...   ..\n",
      "19995  3\n",
      "19996  1\n",
      "19997  2\n",
      "19998  2\n",
      "19999  0\n",
      "\n",
      "[20000 rows x 1 columns]\n",
      "             0         1         2         3         4         5         6   \\\n",
      "0     -0.446891 -0.013397  0.232645  2.156649  1.652923 -0.210531 -0.662227   \n",
      "1      0.243747 -1.144175 -0.622214 -0.661979 -0.373315  0.520313 -1.307954   \n",
      "2     -1.417799 -0.088833 -0.647181 -1.141497 -1.321100  0.686798 -1.045569   \n",
      "3      0.656784  0.381201 -1.039011  1.285315  0.808324  1.428519 -1.144367   \n",
      "4      1.098520  1.706769 -1.030370  2.001009  2.751551  1.202229 -0.941066   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "19995  0.146324 -0.618555  0.547413  0.851301  0.838880 -1.248402  0.698199   \n",
      "19996 -2.227923 -0.327196  1.258849 -0.945254  3.925262  0.316686 -1.061159   \n",
      "19997 -0.800405  1.477997  0.090057 -0.973997 -0.204970  0.224539  0.302265   \n",
      "19998 -1.217344 -0.785401 -0.413781 -0.335324  0.181671 -1.685733 -0.437981   \n",
      "19999  1.601005 -0.890398 -1.425049  0.887501  0.167205 -0.395042 -0.512554   \n",
      "\n",
      "             7         8         9         10        11        12        13  \\\n",
      "0      0.705144  1.434684 -0.445750 -0.178501 -0.905503 -0.594099 -0.728586   \n",
      "1      1.639481 -2.838816  0.331636  0.100261 -1.173067 -0.079774  2.194899   \n",
      "2     -0.376153 -2.272447 -0.669733 -1.070732  1.091993  0.692293  0.937617   \n",
      "3     -0.609958  0.983676  0.060917  0.471647 -0.594279  2.104027 -1.113597   \n",
      "4      1.965282  2.979131  1.402483 -1.432987  0.400322 -0.839851 -1.580974   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "19995 -0.545033  0.994348 -0.054620  0.581239 -1.842074 -0.552879 -0.102096   \n",
      "19996  1.846706  3.692157  0.698035 -0.184239 -0.546036 -0.457919 -1.030581   \n",
      "19997  0.361146 -1.906636  1.381875  0.034366  1.690199 -0.901569  2.183842   \n",
      "19998 -1.176626 -1.434436 -0.880986  2.117703 -2.136842 -1.185261  1.547704   \n",
      "19999  0.050777 -0.034945 -0.289219  0.639189  0.981751 -0.110296 -0.187214   \n",
      "\n",
      "             14        15        16        17        18        19  \n",
      "0     -0.627002 -0.240520  0.131696 -0.945878  1.840922 -0.279024  \n",
      "1      0.242839  0.412164 -0.328714  0.902931 -1.418728 -0.290139  \n",
      "2      0.262142  0.399238  0.614614  1.209465 -1.526386  0.337771  \n",
      "3     -0.188799  1.226253 -1.067763 -0.350275 -0.746426 -1.258696  \n",
      "4     -0.659587  2.292688  0.494188 -1.583187 -2.468254 -0.469648  \n",
      "...         ...       ...       ...       ...       ...       ...  \n",
      "19995  0.307538 -1.267832 -0.165875 -0.717397 -0.808588  0.796346  \n",
      "19996  2.406014 -0.978943 -0.086879 -1.964453 -0.875574  1.535930  \n",
      "19997  0.831008  0.036308  0.516088  0.378670  0.110968 -0.176363  \n",
      "19998  1.174712 -0.429117  0.132068  0.249277 -0.941496  1.661661  \n",
      "19999 -0.226811 -0.547772 -0.006706 -0.023773  1.220809  1.489330  \n",
      "\n",
      "[20000 rows x 20 columns]\n",
      "       0\n",
      "0      3\n",
      "1      2\n",
      "2      2\n",
      "3      3\n",
      "4      3\n",
      "...   ..\n",
      "19995  3\n",
      "19996  1\n",
      "19997  2\n",
      "19998  2\n",
      "19999  0\n",
      "\n",
      "[20000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('C:/Users/bhatt/OneDrive/Desktop/whisper-main/ANN_Data/data1.csv', header=None)\n",
    "print(data)\n",
    "labels=pd.read_csv('C:/Users/bhatt/OneDrive/Desktop/whisper-main/ANN_Data/label1.csv',header=None)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 20)\n",
      "(20000, 1)\n",
      "(20000, 20)\n",
      "(20000, 1)\n"
     ]
    }
   ],
   "source": [
    "data=data.values\n",
    "npLabels=labels.values\n",
    "print(data.shape)\n",
    "print(npLabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "0\n",
      "4\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(np.max(npLabels))\n",
    "print(np.min(npLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(npLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(20000, 1, 5)\n",
      "3\n",
      "(20000, 1, 5)\n"
     ]
    }
   ],
   "source": [
    "num=np.max(npLabels)+1\n",
    "oneHot=np.eye(num)[npLabels]\n",
    "print(oneHot.ndim)\n",
    "print(oneHot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "oneHotRe=oneHot.reshape(20000, 5)\n",
    "print(oneHotRe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData,testDataDash=train_test_split(data,train_size=0.8,test_size=0.2,shuffle=False)\n",
    "trainLabel,testLabelDash=train_test_split(oneHotRe,train_size=0.8,test_size=0.2,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "validData,testData=train_test_split(testDataDash,train_size=0.5,test_size=0.5,shuffle=False)\n",
    "validLabel,testLabel=train_test_split(testLabelDash,train_size=0.5,test_size=0.5,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "b1=np.zeros(h)\n",
    "b2=np.zeros(5)\n",
    "print(b1)\n",
    "print(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1=np.random.normal(0, 1, (20,h))\n",
    "w2=np.random.normal(0, 1, (h,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actFunc(data,choice):\n",
    "\n",
    "    if(choice==1):\n",
    "        return np.tanh(data)\n",
    "    elif(choice==2):\n",
    "        numerator=np.exp(data)\n",
    "        return numerator/np.sum(numerator,axis=1,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunc(y,t):\n",
    "    return -(t*(np.log(y))+(1-t)*np.log(1-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwFunc(data):\n",
    "    a0=data\n",
    "    z1=np.dot(a0,w1)+b1\n",
    "    a1=actFunc(z1,1)\n",
    "    z2=np.dot(a1,w2)+b2\n",
    "    a2=actFunc(z2,2)\n",
    "    return a0,z1,a1,z2,a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Acc(y,t,size):\n",
    "    maxData=np.argmax(y,axis=1)\n",
    "    maxLabel=np.argmax(t,axis=1)\n",
    "    compare=np.equal(maxData,maxLabel)\n",
    "    count=np.sum(compare)\n",
    "    return (count/size)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cost 1.4119220439843931\n",
      "Training accuracy 68.86875\n",
      "Training Cost 1.2308233546631235\n",
      "Training accuracy 73.6625\n",
      "Training Cost 1.4119220439843931\n",
      "Training accuracy 68.86875\n",
      "Training Cost 1.2308233546631235\n",
      "Training accuracy 73.6625\n",
      "Training Cost 1.120950376032111\n",
      "Training accuracy 76.3875\n",
      "Training Cost 1.013448143153476\n",
      "Training accuracy 79.04375\n",
      "Training Cost 0.9338755762144504\n",
      "Training accuracy 80.84375\n",
      "Training Cost 0.8815391839380816\n",
      "Training accuracy 82.28750000000001\n",
      "Training Cost 0.8423042659232114\n",
      "Training accuracy 83.26875\n",
      "Training Cost 0.8031876437968242\n",
      "Training accuracy 84.30625\n",
      "Training Cost 0.7643627731034763\n",
      "Training accuracy 85.2375\n",
      "Training Cost 1.120950376032111\n",
      "Training accuracy 76.3875\n",
      "Training Cost 1.013448143153476\n",
      "Training accuracy 79.04375\n",
      "Training Cost 0.9338755762144504\n",
      "Training accuracy 80.84375\n",
      "Training Cost 0.8815391839380816\n",
      "Training accuracy 82.28750000000001\n",
      "Training Cost 0.8423042659232114\n",
      "Training accuracy 83.26875\n",
      "Training Cost 0.8031876437968242\n",
      "Training accuracy 84.30625\n",
      "Training Cost 0.7643627731034763\n",
      "Training accuracy 85.2375\n",
      "Training Cost 0.74032266335399\n",
      "Training accuracy 85.85625\n",
      "Training Cost 0.7237429731472118\n",
      "Training accuracy 86.36875\n",
      "Training Cost 0.74032266335399\n",
      "Training accuracy 85.85625\n",
      "Training Cost 0.7237429731472118\n",
      "Training accuracy 86.36875\n",
      "Training Cost 0.8020757641056518\n",
      "Training accuracy 84.89999999999999\n",
      "Training Cost 0.7779092763722321\n",
      "Training accuracy 85.625\n",
      "Training Cost 0.7538088463060723\n",
      "Training accuracy 86.19375000000001\n",
      "Training Cost 0.7492361432974854\n",
      "Training accuracy 86.4875\n",
      "Training Cost 0.7337818613119033\n",
      "Training accuracy 86.7\n",
      "Training Cost 0.7324239550984528\n",
      "Training accuracy 86.675\n",
      "Training Cost 0.8020757641056518\n",
      "Training accuracy 84.89999999999999\n",
      "Training Cost 0.7779092763722321\n",
      "Training accuracy 85.625\n",
      "Training Cost 0.7538088463060723\n",
      "Training accuracy 86.19375000000001\n",
      "Training Cost 0.7492361432974854\n",
      "Training accuracy 86.4875\n",
      "Training Cost 0.7337818613119033\n",
      "Training accuracy 86.7\n",
      "Training Cost 0.7324239550984528\n",
      "Training accuracy 86.675\n",
      "Training Cost 0.6879437004136756\n",
      "Training accuracy 87.26875\n",
      "Training Cost 0.6534683092065241\n",
      "Training accuracy 87.925\n",
      "Training Cost 0.6332029784974871\n",
      "Training accuracy 88.31875\n",
      "Training Cost 0.6879437004136756\n",
      "Training accuracy 87.26875\n",
      "Training Cost 0.6534683092065241\n",
      "Training accuracy 87.925\n",
      "Training Cost 0.6332029784974871\n",
      "Training accuracy 88.31875\n",
      "Training Cost 0.61966454108196\n",
      "Training accuracy 88.64375\n",
      "Training Cost 0.6091574710048477\n",
      "Training accuracy 88.86874999999999\n",
      "Training Cost 0.6011734087034477\n",
      "Training accuracy 88.97500000000001\n",
      "Training Cost 0.5949339817139058\n",
      "Training accuracy 89.09375\n",
      "Training Cost 0.5899842934194384\n",
      "Training accuracy 89.275\n",
      "Training Cost 0.5859716313226078\n",
      "Training accuracy 89.43124999999999\n",
      "Training Cost 0.61966454108196\n",
      "Training accuracy 88.64375\n",
      "Training Cost 0.6091574710048477\n",
      "Training accuracy 88.86874999999999\n",
      "Training Cost 0.6011734087034477\n",
      "Training accuracy 88.97500000000001\n",
      "Training Cost 0.5949339817139058\n",
      "Training accuracy 89.09375\n",
      "Training Cost 0.5899842934194384\n",
      "Training accuracy 89.275\n",
      "Training Cost 0.5859716313226078\n",
      "Training accuracy 89.43124999999999\n",
      "Training Cost 0.5825985859312527\n",
      "Training accuracy 89.45625\n",
      "Training Cost 0.5796460520653974\n",
      "Training accuracy 89.53750000000001\n",
      "Training Cost 0.5769773552958154\n",
      "Training accuracy 89.63125\n",
      "Training Cost 0.5745238277539083\n",
      "Training Cost 0.5825985859312527\n",
      "Training accuracy 89.45625\n",
      "Training Cost 0.5796460520653974\n",
      "Training accuracy 89.53750000000001\n",
      "Training Cost 0.5769773552958154\n",
      "Training accuracy 89.63125\n",
      "Training Cost 0.5745238277539083\n",
      "Training accuracy 89.625\n",
      "Training Cost 0.5722616222012588\n",
      "Training accuracy 89.6375\n",
      "Training Cost 0.5701868389232388\n",
      "Training accuracy 89.6625\n",
      "Training Cost 0.5682940862540548\n",
      "Training accuracy 89.71875\n",
      "Training Cost 0.5665637231537293\n",
      "Training accuracy 89.75\n",
      "Training accuracy 89.625\n",
      "Training Cost 0.5722616222012588\n",
      "Training accuracy 89.6375\n",
      "Training Cost 0.5701868389232388\n",
      "Training accuracy 89.6625\n",
      "Training Cost 0.5682940862540548\n",
      "Training accuracy 89.71875\n",
      "Training Cost 0.5665637231537293\n",
      "Training accuracy 89.75\n",
      "Training Cost 0.5649640733834314\n",
      "Training accuracy 89.78125\n",
      "Training Cost 0.5634613143107403\n",
      "Training accuracy 89.8375\n",
      "Training Cost 0.5620273109276319\n",
      "Training accuracy 89.9\n",
      "Training Cost 0.5606419180341645\n",
      "Training accuracy 89.8875\n",
      "Training Cost 0.5649640733834314\n",
      "Training accuracy 89.78125\n",
      "Training Cost 0.5634613143107403\n",
      "Training accuracy 89.8375\n",
      "Training Cost 0.5620273109276319\n",
      "Training accuracy 89.9\n",
      "Training Cost 0.5606419180341645\n",
      "Training accuracy 89.8875\n",
      "Training Cost 0.5592896886918471\n",
      "Training accuracy 89.8625\n",
      "Training Cost 0.5579563715911436\n",
      "Training accuracy 89.86874999999999\n",
      "Training Cost 0.5566278213017197\n",
      "Training accuracy 89.81875000000001\n",
      "Training Cost 0.5552893442725533\n",
      "Training accuracy 89.83125\n",
      "Training Cost 0.5539244466842028\n",
      "Training accuracy 89.85\n",
      "Training Cost 0.5592896886918471\n",
      "Training accuracy 89.8625\n",
      "Training Cost 0.5579563715911436\n",
      "Training accuracy 89.86874999999999\n",
      "Training Cost 0.5566278213017197\n",
      "Training accuracy 89.81875000000001\n",
      "Training Cost 0.5552893442725533\n",
      "Training accuracy 89.83125\n",
      "Training Cost 0.5539244466842028\n",
      "Training accuracy 89.85\n",
      "Training Cost 0.5525145797835047\n",
      "Training accuracy 89.88125\n",
      "Training Cost 0.5510449797701884\n",
      "Training accuracy 89.93124999999999\n",
      "Training Cost 0.5495190401357611\n",
      "Training accuracy 89.95\n",
      "Training Cost 0.5479630932297751\n",
      "Training accuracy 89.95625000000001\n",
      "Training Cost 0.5525145797835047\n",
      "Training accuracy 89.88125\n",
      "Training Cost 0.5510449797701884\n",
      "Training accuracy 89.93124999999999\n",
      "Training Cost 0.5495190401357611\n",
      "Training accuracy 89.95\n",
      "Training Cost 0.5479630932297751\n",
      "Training accuracy 89.95625000000001\n",
      "Training Cost 0.5464031799833751\n",
      "Training accuracy 90.01249999999999\n",
      "Training Cost 0.5448434438664634\n",
      "Training accuracy 90.03125\n",
      "Training Cost 0.5432643844013446\n",
      "Training accuracy 90.04375\n",
      "Training Cost 0.541645816772636\n",
      "Training accuracy 90.0625\n",
      "Training Cost 0.5400041917209103\n",
      "Training accuracy 90.10625\n",
      "Training Cost 0.5464031799833751\n",
      "Training accuracy 90.01249999999999\n",
      "Training Cost 0.5448434438664634\n",
      "Training accuracy 90.03125\n",
      "Training Cost 0.5432643844013446\n",
      "Training accuracy 90.04375\n",
      "Training Cost 0.541645816772636\n",
      "Training accuracy 90.0625\n",
      "Training Cost 0.5400041917209103\n",
      "Training accuracy 90.10625\n",
      "Training Cost 0.5384036635744909\n",
      "Training accuracy 90.08749999999999\n",
      "Training Cost 0.5369041837379422\n",
      "Training accuracy 90.11875\n",
      "Training Cost 0.5355086897722888\n",
      "Training accuracy 90.17500000000001\n",
      "Training Cost 0.5341908611456024\n",
      "Training accuracy 90.18125\n",
      "Training Cost 0.5384036635744909\n",
      "Training accuracy 90.08749999999999\n",
      "Training Cost 0.5369041837379422\n",
      "Training accuracy 90.11875\n",
      "Training Cost 0.5355086897722888\n",
      "Training accuracy 90.17500000000001\n",
      "Training Cost 0.5341908611456024\n",
      "Training accuracy 90.18125\n",
      "Training Cost 0.5329300946596869\n",
      "Training accuracy 90.28125\n",
      "Training Cost 0.5317179383913972\n",
      "Training accuracy 90.325\n",
      "Training Cost 0.5305463453725148\n",
      "Training accuracy 90.36874999999999\n",
      "Training Cost 0.5293990432600862\n",
      "Training accuracy 90.34375\n",
      "Training Cost 0.5282525715666678\n",
      "Training accuracy 90.39375\n",
      "Training Cost 0.5329300946596869\n",
      "Training accuracy 90.28125\n",
      "Training Cost 0.5317179383913972\n",
      "Training accuracy 90.325\n",
      "Training Cost 0.5305463453725148\n",
      "Training accuracy 90.36874999999999\n",
      "Training Cost 0.5293990432600862\n",
      "Training accuracy 90.34375\n",
      "Training Cost 0.5282525715666678\n",
      "Training accuracy 90.39375\n",
      "Training Cost 0.5270837220050242\n",
      "Training accuracy 90.39375\n",
      "Training Cost 0.5258815080174795\n",
      "Training accuracy 90.4125\n",
      "Training Cost 0.5246549016307833\n",
      "Training accuracy 90.4\n",
      "Training Cost 0.5234231328182611\n",
      "Training accuracy 90.38125000000001\n",
      "Training Cost 0.5270837220050242\n",
      "Training accuracy 90.39375\n",
      "Training Cost 0.5258815080174795\n",
      "Training accuracy 90.4125\n",
      "Training Cost 0.5246549016307833\n",
      "Training accuracy 90.4\n",
      "Training Cost 0.5234231328182611\n",
      "Training accuracy 90.38125000000001\n",
      "Training Cost 0.5221986985383539\n",
      "Training accuracy 90.41875\n",
      "Training Cost 0.5209833321906487\n",
      "Training accuracy 90.5\n",
      "Training Cost 0.5197725388288386\n",
      "Training accuracy 90.55\n",
      "Training Cost 0.5185583711890661\n",
      "Training accuracy 90.55\n",
      "Training Cost 0.5173299462818626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cost 0.5221986985383539\n",
      "Training accuracy 90.41875\n",
      "Training Cost 0.5209833321906487\n",
      "Training accuracy 90.5\n",
      "Training Cost 0.5197725388288386\n",
      "Training accuracy 90.55\n",
      "Training Cost 0.5185583711890661\n",
      "Training accuracy 90.55\n",
      "Training Cost 0.5173299462818626\n",
      "Training accuracy 90.55\n",
      "Training Cost 0.5160734095284737\n",
      "Training accuracy 90.56875\n",
      "Training Cost 0.514767902608739\n",
      "Training accuracy 90.64375\n",
      "Training Cost 0.513380890501228\n",
      "Training accuracy 90.69375\n",
      "Training Cost 0.5118965480264256\n",
      "Training accuracy 90.68125\n",
      "Training accuracy 90.55\n",
      "Training Cost 0.5160734095284737\n",
      "Training accuracy 90.56875\n",
      "Training Cost 0.514767902608739\n",
      "Training accuracy 90.64375\n",
      "Training Cost 0.513380890501228\n",
      "Training accuracy 90.69375\n",
      "Training Cost 0.5118965480264256\n",
      "Training accuracy 90.68125\n",
      "Training Cost 0.5103692806979706\n",
      "Training accuracy 90.725\n",
      "Training Cost 0.5088893408965263\n",
      "Training accuracy 90.77499999999999\n",
      "Training Cost 0.5075153788667544\n",
      "Training accuracy 90.77499999999999\n",
      "Training Cost 0.5062460153822586\n",
      "Training accuracy 90.81875000000001\n",
      "Training Cost 0.5103692806979706\n",
      "Training accuracy 90.725\n",
      "Training Cost 0.5088893408965263\n",
      "Training accuracy 90.77499999999999\n",
      "Training Cost 0.5075153788667544\n",
      "Training accuracy 90.77499999999999\n",
      "Training Cost 0.5062460153822586\n",
      "Training accuracy 90.81875000000001\n",
      "Training Cost 0.5050505168744193\n",
      "Training accuracy 90.8875\n",
      "Training Cost 0.5039147819853577\n",
      "Training accuracy 90.9125\n",
      "Training Cost 0.502840460533322\n",
      "Training accuracy 90.9\n",
      "Training Cost 0.5018296367671989\n",
      "Training accuracy 90.90625\n",
      "Training Cost 0.5008848272657642\n",
      "Training accuracy 90.95625000000001\n",
      "Training Cost 0.5050505168744193\n",
      "Training accuracy 90.8875\n",
      "Training Cost 0.5039147819853577\n",
      "Training accuracy 90.9125\n",
      "Training Cost 0.502840460533322\n",
      "Training accuracy 90.9\n",
      "Training Cost 0.5018296367671989\n",
      "Training accuracy 90.90625\n",
      "Training Cost 0.5008848272657642\n",
      "Training accuracy 90.95625000000001\n",
      "Training Cost 0.5000066105204454\n",
      "Training accuracy 90.95625000000001\n",
      "Training Cost 0.4991912038133851\n",
      "Training accuracy 90.9375\n",
      "Training Cost 0.49843114519060155\n",
      "Training accuracy 90.9375\n",
      "Training Cost 0.49771895439407715\n",
      "Training accuracy 90.9875\n",
      "Training Cost 0.5000066105204454\n",
      "Training accuracy 90.95625000000001\n",
      "Training Cost 0.4991912038133851\n",
      "Training accuracy 90.9375\n",
      "Training Cost 0.49843114519060155\n",
      "Training accuracy 90.9375\n",
      "Training Cost 0.49771895439407715\n",
      "Training accuracy 90.9875\n",
      "Training Cost 0.4970519032282814\n",
      "Training accuracy 91.03125\n",
      "Training Cost 0.4964317026377404\n",
      "Training accuracy 91.0625\n",
      "Training Cost 0.49585910903936803\n",
      "Training accuracy 91.0125\n",
      "Training Cost 0.4953311175282474\n",
      "Training accuracy 91.04375\n",
      "Training Cost 0.4948427592401179\n",
      "Training accuracy 91.01875\n",
      "Training Cost 0.4970519032282814\n",
      "Training accuracy 91.03125\n",
      "Training Cost 0.4964317026377404\n",
      "Training accuracy 91.0625\n",
      "Training Cost 0.49585910903936803\n",
      "Training accuracy 91.0125\n",
      "Training Cost 0.4953311175282474\n",
      "Training accuracy 91.04375\n",
      "Training Cost 0.4948427592401179\n",
      "Training accuracy 91.01875\n",
      "Training Cost 0.49438986820078046\n",
      "Training accuracy 91.05625\n",
      "Training Cost 0.49396999786956397\n",
      "Training accuracy 91.06875\n",
      "Training Cost 0.4935812949505342\n",
      "Training accuracy 91.06875\n",
      "Training Cost 0.49322089000860525\n",
      "Training accuracy 91.08125\n",
      "Training Cost 0.49438986820078046\n",
      "Training accuracy 91.05625\n",
      "Training Cost 0.49396999786956397\n",
      "Training accuracy 91.06875\n",
      "Training Cost 0.4935812949505342\n",
      "Training accuracy 91.06875\n",
      "Training Cost 0.49322089000860525\n",
      "Training accuracy 91.08125\n",
      "Training Cost 0.4928841799378142\n",
      "Training accuracy 91.10625\n",
      "Training Cost 0.49256499870016796\n",
      "Training accuracy 91.10000000000001\n",
      "Training Cost 0.4922558995945187\n",
      "Training accuracy 91.125\n",
      "Training Cost 0.49194797002958657\n",
      "Training accuracy 91.1375\n",
      "Training Cost 0.4916299669396124\n",
      "Training accuracy 91.1375\n",
      "Training Cost 0.4928841799378142\n",
      "Training accuracy 91.10625\n",
      "Training Cost 0.49256499870016796\n",
      "Training accuracy 91.10000000000001\n",
      "Training Cost 0.4922558995945187\n",
      "Training accuracy 91.125\n",
      "Training Cost 0.49194797002958657\n",
      "Training accuracy 91.1375\n",
      "Training Cost 0.4916299669396124\n",
      "Training accuracy 91.1375\n",
      "Training Cost 0.4912866452292188\n",
      "Training accuracy 91.125\n",
      "Training Cost 0.49089608253641803\n",
      "Training accuracy 91.11875\n",
      "Training Cost 0.49042588960650546\n",
      "Training accuracy 91.1375\n",
      "Training Cost 0.48982838795934003\n",
      "Training accuracy 91.1875\n",
      "Training Cost 0.4912866452292188\n",
      "Training accuracy 91.125\n",
      "Training Cost 0.49089608253641803\n",
      "Training accuracy 91.11875\n",
      "Training Cost 0.49042588960650546\n",
      "Training accuracy 91.1375\n",
      "Training Cost 0.48982838795934003\n",
      "Training accuracy 91.1875\n",
      "Training Cost 0.4890350350636371\n",
      "Training accuracy 91.18125\n",
      "Training Cost 0.48796623933528566\n",
      "Training accuracy 91.16875\n",
      "Training Cost 0.48666075331922987\n",
      "Training accuracy 91.1875\n",
      "Training Cost 0.4854364381004863\n",
      "Training accuracy 91.1875\n",
      "Training Cost 0.4845204297938946\n",
      "Training accuracy 91.175\n",
      "Training Cost 0.4890350350636371\n",
      "Training accuracy 91.18125\n",
      "Training Cost 0.48796623933528566\n",
      "Training accuracy 91.16875\n",
      "Training Cost 0.48666075331922987\n",
      "Training accuracy 91.1875\n",
      "Training Cost 0.4854364381004863\n",
      "Training accuracy 91.1875\n",
      "Training Cost 0.4845204297938946\n",
      "Training accuracy 91.175\n",
      "Training Cost 0.4838612570070926\n",
      "Training accuracy 91.225\n",
      "Training Cost 0.4833539568980691\n",
      "Training accuracy 91.28125\n",
      "Training Cost 0.48293328325976126\n",
      "Training accuracy 91.27499999999999\n",
      "Training Cost 0.48256758525333204\n",
      "Training accuracy 91.2625\n",
      "Training Cost 0.4838612570070926\n",
      "Training accuracy 91.225\n",
      "Training Cost 0.4833539568980691\n",
      "Training accuracy 91.28125\n",
      "Training Cost 0.48293328325976126\n",
      "Training accuracy 91.27499999999999\n",
      "Training Cost 0.48256758525333204\n",
      "Training accuracy 91.2625\n",
      "Training Cost 0.48224149922556686\n",
      "Training accuracy 91.31875000000001\n",
      "Training Cost 0.4819467366587679\n",
      "Training accuracy 91.3125\n",
      "Training Cost 0.4816780989849628\n",
      "Training accuracy 91.31875000000001\n",
      "Training Cost 0.4814318307898619\n",
      "Training accuracy 91.34375\n",
      "Training Cost 0.4812049278174508\n",
      "Training accuracy 91.35\n",
      "Training Cost 0.48224149922556686\n",
      "Training accuracy 91.31875000000001\n",
      "Training Cost 0.4819467366587679\n",
      "Training accuracy 91.3125\n",
      "Training Cost 0.4816780989849628\n",
      "Training accuracy 91.31875000000001\n",
      "Training Cost 0.4814318307898619\n",
      "Training accuracy 91.34375\n",
      "Training Cost 0.4812049278174508\n",
      "Training accuracy 91.35\n",
      "Training Cost 0.48099483552525535\n",
      "Training accuracy 91.3375\n",
      "Training Cost 0.4807993092076651\n",
      "Training accuracy 91.3375\n",
      "Training Cost 0.4806163431692886\n",
      "Training accuracy 91.31875000000001\n",
      "Training Cost 0.4804441309789595\n",
      "Training accuracy 91.35\n",
      "Training Cost 0.48099483552525535\n",
      "Training accuracy 91.3375\n",
      "Training Cost 0.4807993092076651\n",
      "Training accuracy 91.3375\n",
      "Training Cost 0.4806163431692886\n",
      "Training accuracy 91.31875000000001\n",
      "Training Cost 0.4804441309789595\n",
      "Training accuracy 91.35\n",
      "Training Cost 0.4802810399396788\n",
      "Training accuracy 91.3875\n",
      "Training Cost 0.48012559084093004\n",
      "Training accuracy 91.39375\n",
      "Training Cost 0.47997643781405874\n",
      "Training accuracy 91.3875\n",
      "Training Cost 0.4798323458194854\n",
      "Training accuracy 91.3875\n",
      "Training Cost 0.47969216555229155\n",
      "Training accuracy 91.4\n",
      "Training Cost 0.4795548073618755\n",
      "Training Cost 0.4802810399396788\n",
      "Training accuracy 91.3875\n",
      "Training Cost 0.48012559084093004\n",
      "Training accuracy 91.39375\n",
      "Training Cost 0.47997643781405874\n",
      "Training accuracy 91.3875\n",
      "Training Cost 0.4798323458194854\n",
      "Training accuracy 91.3875\n",
      "Training Cost 0.47969216555229155\n",
      "Training accuracy 91.4\n",
      "Training Cost 0.4795548073618755\n",
      "Training accuracy 91.425\n",
      "Training Cost 0.479419217283654\n",
      "Training accuracy 91.41875\n",
      "Training Cost 0.479284359765524\n",
      "Training accuracy 91.43124999999999\n",
      "Training Cost 0.47914921328580323\n",
      "Training accuracy 91.425\n",
      "Training accuracy 91.425\n",
      "Training Cost 0.479419217283654\n",
      "Training accuracy 91.41875\n",
      "Training Cost 0.479284359765524\n",
      "Training accuracy 91.43124999999999\n",
      "Training Cost 0.47914921328580323\n",
      "Training accuracy 91.425\n",
      "Training Cost 0.4790127864309811\n",
      "Training accuracy 91.41875\n",
      "Training Cost 0.47887416204217653\n",
      "Training accuracy 91.43124999999999\n",
      "Training Cost 0.4787325743888347\n",
      "Training accuracy 91.4375\n",
      "Training Cost 0.4785875183228987\n",
      "Training accuracy 91.43124999999999\n",
      "Training Cost 0.4790127864309811\n",
      "Training accuracy 91.41875\n",
      "Training Cost 0.47887416204217653\n",
      "Training accuracy 91.43124999999999\n",
      "Training Cost 0.4787325743888347\n",
      "Training accuracy 91.4375\n",
      "Training Cost 0.4785875183228987\n",
      "Training accuracy 91.43124999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cost 0.47843887897369597\n",
      "Training accuracy 91.4375\n",
      "Training Cost 0.47828704862848714\n",
      "Training accuracy 91.475\n",
      "Training Cost 0.4781329563133668\n",
      "Training accuracy 91.475\n",
      "Training Cost 0.4779779186997235\n",
      "Training accuracy 91.46875\n",
      "Training Cost 0.4778233306992409\n",
      "Training accuracy 91.46875\n",
      "Training Cost 0.47843887897369597\n",
      "Training accuracy 91.4375\n",
      "Training Cost 0.47828704862848714\n",
      "Training accuracy 91.475\n",
      "Training Cost 0.4781329563133668\n",
      "Training accuracy 91.475\n",
      "Training Cost 0.4779779186997235\n",
      "Training accuracy 91.46875\n",
      "Training Cost 0.4778233306992409\n",
      "Training accuracy 91.46875\n",
      "Training Cost 0.4776703837769048\n",
      "Training accuracy 91.46875\n",
      "Training Cost 0.4775199498907695\n",
      "Training accuracy 91.50625\n",
      "Training Cost 0.47737254552885167\n",
      "Training accuracy 91.53125\n",
      "Training Cost 0.47722824093039\n",
      "Training accuracy 91.53750000000001\n",
      "Training Cost 0.4776703837769048\n",
      "Training accuracy 91.46875\n",
      "Training Cost 0.4775199498907695\n",
      "Training accuracy 91.50625\n",
      "Training Cost 0.47737254552885167\n",
      "Training accuracy 91.53125\n",
      "Training Cost 0.47722824093039\n",
      "Training accuracy 91.53750000000001\n",
      "Training Cost 0.4770865198344111\n",
      "Training accuracy 91.5625\n",
      "Training Cost 0.4769462147425718\n",
      "Training accuracy 91.58125\n",
      "Training Cost 0.4768056398431392\n",
      "Training accuracy 91.59375\n",
      "Training Cost 0.47666291119769205\n",
      "Training accuracy 91.57499999999999\n",
      "Training Cost 0.47651628100321874\n",
      "Training accuracy 91.58125\n",
      "Training Cost 0.4770865198344111\n",
      "Training accuracy 91.5625\n",
      "Training Cost 0.4769462147425718\n",
      "Training accuracy 91.58125\n",
      "Training Cost 0.4768056398431392\n",
      "Training accuracy 91.59375\n",
      "Training Cost 0.47666291119769205\n",
      "Training accuracy 91.57499999999999\n",
      "Training Cost 0.47651628100321874\n",
      "Training accuracy 91.58125\n",
      "Training Cost 0.47636431352931063\n",
      "Training accuracy 91.5875\n",
      "Training Cost 0.4762059148596708\n",
      "Training accuracy 91.55\n",
      "Training Cost 0.47604032776607874\n",
      "Training accuracy 91.54375\n",
      "Training Cost 0.4758670873107231\n",
      "Training accuracy 91.55624999999999\n",
      "Training Cost 0.47636431352931063\n",
      "Training accuracy 91.5875\n",
      "Training Cost 0.4762059148596708\n",
      "Training accuracy 91.55\n",
      "Training Cost 0.47604032776607874\n",
      "Training accuracy 91.54375\n",
      "Training Cost 0.4758670873107231\n",
      "Training accuracy 91.55624999999999\n",
      "Training Cost 0.47568584260834035\n",
      "Training accuracy 91.56875\n",
      "Training Cost 0.4754960568655744\n",
      "Training accuracy 91.57499999999999\n",
      "Training Cost 0.47529674113804127\n",
      "Training accuracy 91.57499999999999\n",
      "Training Cost 0.4750863829948247\n",
      "Training accuracy 91.5625\n",
      "Training Cost 0.47568584260834035\n",
      "Training accuracy 91.56875\n",
      "Training Cost 0.4754960568655744\n",
      "Training accuracy 91.57499999999999\n",
      "Training Cost 0.47529674113804127\n",
      "Training accuracy 91.57499999999999\n",
      "Training Cost 0.4750863829948247\n",
      "Training accuracy 91.5625\n",
      "Training Cost 0.4748631882108558\n",
      "Training accuracy 91.54375\n",
      "Training Cost 0.4746257996544399\n",
      "Training accuracy 91.54375\n",
      "Training Cost 0.4743746346979894\n",
      "Training accuracy 91.54375\n",
      "Training Cost 0.474113407616582\n",
      "Training accuracy 91.55624999999999\n",
      "Training Cost 0.47384928574579555\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.47359048680409743\n",
      "Training accuracy 91.49374999999999\n",
      "Training Cost 0.4748631882108558\n",
      "Training accuracy 91.54375\n",
      "Training Cost 0.4746257996544399\n",
      "Training accuracy 91.54375\n",
      "Training Cost 0.4743746346979894\n",
      "Training accuracy 91.54375\n",
      "Training Cost 0.474113407616582\n",
      "Training accuracy 91.55624999999999\n",
      "Training Cost 0.47384928574579555\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.47359048680409743\n",
      "Training accuracy 91.49374999999999\n",
      "Training Cost 0.4733431873949772\n",
      "Training accuracy 91.4625\n",
      "Training Cost 0.47311062807707305\n",
      "Training accuracy 91.44375\n",
      "Training Cost 0.472894036259334\n",
      "Training accuracy 91.425\n",
      "Training Cost 0.4733431873949772\n",
      "Training accuracy 91.4625\n",
      "Training Cost 0.47311062807707305\n",
      "Training accuracy 91.44375\n",
      "Training Cost 0.472894036259334\n",
      "Training accuracy 91.425\n",
      "Training Cost 0.47269339536694527\n",
      "Training accuracy 91.3875\n",
      "Training Cost 0.4725076239716546\n",
      "Training accuracy 91.4125\n",
      "Training Cost 0.4723347349865239\n",
      "Training accuracy 91.44375\n",
      "Training Cost 0.47217223462261015\n",
      "Training accuracy 91.44375\n",
      "Training Cost 0.4720175876233297\n",
      "Training accuracy 91.4375\n",
      "Training Cost 0.47269339536694527\n",
      "Training accuracy 91.3875\n",
      "Training Cost 0.4725076239716546\n",
      "Training accuracy 91.4125\n",
      "Training Cost 0.4723347349865239\n",
      "Training accuracy 91.44375\n",
      "Training Cost 0.47217223462261015\n",
      "Training accuracy 91.44375\n",
      "Training Cost 0.4720175876233297\n",
      "Training accuracy 91.4375\n",
      "Training Cost 0.4718685511617755\n",
      "Training accuracy 91.46875\n",
      "Training Cost 0.47172333700975216\n",
      "Training accuracy 91.4625\n",
      "Training Cost 0.471580670157618\n",
      "Training accuracy 91.45\n",
      "Training Cost 0.47143984988686716\n",
      "Training accuracy 91.45\n",
      "Training Cost 0.4718685511617755\n",
      "Training accuracy 91.46875\n",
      "Training Cost 0.47172333700975216\n",
      "Training accuracy 91.4625\n",
      "Training Cost 0.471580670157618\n",
      "Training accuracy 91.45\n",
      "Training Cost 0.47143984988686716\n",
      "Training accuracy 91.45\n",
      "Training Cost 0.4713007961311341\n",
      "Training accuracy 91.45\n",
      "Training Cost 0.47116388827573424\n",
      "Training accuracy 91.44375\n",
      "Training Cost 0.4710295699458798\n",
      "Training accuracy 91.44375\n",
      "Training Cost 0.4708980076992233\n",
      "Training accuracy 91.4625\n",
      "Training Cost 0.4707690208149716\n",
      "Training accuracy 91.49374999999999\n",
      "Training Cost 0.4713007961311341\n",
      "Training accuracy 91.45\n",
      "Training Cost 0.47116388827573424\n",
      "Training accuracy 91.44375\n",
      "Training Cost 0.4710295699458798\n",
      "Training accuracy 91.44375\n",
      "Training Cost 0.4708980076992233\n",
      "Training accuracy 91.4625\n",
      "Training Cost 0.4707690208149716\n",
      "Training accuracy 91.49374999999999\n",
      "Training Cost 0.47064221747295476\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.4705171776316615\n",
      "Training accuracy 91.51875000000001\n",
      "Training Cost 0.4703935943519581\n",
      "Training accuracy 91.53125\n",
      "Training Cost 0.47027135933511305\n",
      "Training accuracy 91.54375\n",
      "Training Cost 0.47064221747295476\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.4705171776316615\n",
      "Training accuracy 91.51875000000001\n",
      "Training Cost 0.4703935943519581\n",
      "Training accuracy 91.53125\n",
      "Training Cost 0.47027135933511305\n",
      "Training accuracy 91.54375\n",
      "Training Cost 0.4701506035175452\n",
      "Training accuracy 91.53125\n",
      "Training Cost 0.4700316995481438\n",
      "Training accuracy 91.54375\n",
      "Training Cost 0.4699152235728932\n",
      "Training accuracy 91.54375\n",
      "Training Cost 0.46980187183773997\n",
      "Training accuracy 91.55\n",
      "Training Cost 0.4696923383534015\n",
      "Training accuracy 91.5625\n",
      "Training Cost 0.4701506035175452\n",
      "Training accuracy 91.53125\n",
      "Training Cost 0.4700316995481438\n",
      "Training accuracy 91.54375\n",
      "Training Cost 0.4699152235728932\n",
      "Training accuracy 91.54375\n",
      "Training Cost 0.46980187183773997\n",
      "Training accuracy 91.55\n",
      "Training Cost 0.4696923383534015\n",
      "Training accuracy 91.5625\n",
      "Training Cost 0.4695871797592961\n",
      "Training accuracy 91.5625\n",
      "Training Cost 0.46948670786651087\n",
      "Training accuracy 91.55\n",
      "Training Cost 0.46939094269453324\n",
      "Training accuracy 91.58125\n",
      "Training Cost 0.4692996303585927\n",
      "Training accuracy 91.58125\n",
      "Training Cost 0.4692123022125852\n",
      "Training accuracy 91.57499999999999\n",
      "Training Cost 0.4695871797592961\n",
      "Training accuracy 91.5625\n",
      "Training Cost 0.46948670786651087\n",
      "Training accuracy 91.55\n",
      "Training Cost 0.46939094269453324\n",
      "Training accuracy 91.58125\n",
      "Training Cost 0.4692996303585927\n",
      "Training accuracy 91.58125\n",
      "Training Cost 0.4692123022125852\n",
      "Training accuracy 91.57499999999999\n",
      "Training Cost 0.4691283431741139\n",
      "Training accuracy 91.56875\n",
      "Training Cost 0.46904704739927056\n",
      "Training accuracy 91.56875\n",
      "Training Cost 0.46896765513093736\n",
      "Training accuracy 91.60000000000001\n",
      "Training Cost 0.46888937564850314\n",
      "Training accuracy 91.59375\n",
      "Training Cost 0.4691283431741139\n",
      "Training accuracy 91.56875\n",
      "Training Cost 0.46904704739927056\n",
      "Training accuracy 91.56875\n",
      "Training Cost 0.46896765513093736\n",
      "Training accuracy 91.60000000000001\n",
      "Training Cost 0.46888937564850314\n",
      "Training accuracy 91.59375\n",
      "Training Cost 0.46881140636235114\n",
      "Training accuracy 91.5625\n",
      "Training Cost 0.46873295985569674\n",
      "Training accuracy 91.59375\n",
      "Training Cost 0.4686533110865454\n",
      "Training accuracy 91.5875\n",
      "Training Cost 0.46857187580431364\n",
      "Training accuracy 91.58125\n",
      "Training Cost 0.4684883258326064\n",
      "Training accuracy 91.58125\n",
      "Training Cost 0.46881140636235114\n",
      "Training accuracy 91.5625\n",
      "Training Cost 0.46873295985569674\n",
      "Training accuracy 91.59375\n",
      "Training Cost 0.4686533110865454\n",
      "Training accuracy 91.5875\n",
      "Training Cost 0.46857187580431364\n",
      "Training accuracy 91.58125\n",
      "Training Cost 0.4684883258326064\n",
      "Training accuracy 91.58125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cost 0.46840273218111045\n",
      "Training accuracy 91.58125\n",
      "Training Cost 0.4683156996729441\n",
      "Training accuracy 91.57499999999999\n",
      "Training Cost 0.46822842825341515\n",
      "Training accuracy 91.56875\n",
      "Training Cost 0.46814263474877\n",
      "Training accuracy 91.56875\n",
      "Training Cost 0.46840273218111045\n",
      "Training accuracy 91.58125\n",
      "Training Cost 0.4683156996729441\n",
      "Training accuracy 91.57499999999999\n",
      "Training Cost 0.46822842825341515\n",
      "Training accuracy 91.56875\n",
      "Training Cost 0.46814263474877\n",
      "Training accuracy 91.56875\n",
      "Training Cost 0.46806030695987216\n",
      "Training accuracy 91.55624999999999\n",
      "Training Cost 0.4679833305089958\n",
      "Training accuracy 91.5625\n",
      "Training Cost 0.467913108165423\n",
      "Training accuracy 91.55\n",
      "Training Cost 0.4678502971978792\n",
      "Training accuracy 91.55\n",
      "Training Cost 0.46806030695987216\n",
      "Training accuracy 91.55624999999999\n",
      "Training Cost 0.4679833305089958\n",
      "Training accuracy 91.5625\n",
      "Training Cost 0.467913108165423\n",
      "Training accuracy 91.55\n",
      "Training Cost 0.4678502971978792\n",
      "Training accuracy 91.55\n",
      "Training Cost 0.46779469823692293\n",
      "Training accuracy 91.53125\n",
      "Training Cost 0.46774527800379456\n",
      "Training accuracy 91.55\n",
      "Training Cost 0.46770032838717673\n",
      "Training accuracy 91.55624999999999\n",
      "Training Cost 0.46765774538861443\n",
      "Training accuracy 91.55\n",
      "Training Cost 0.4676153557841491\n",
      "Training accuracy 91.57499999999999\n",
      "Training Cost 0.46779469823692293\n",
      "Training accuracy 91.53125\n",
      "Training Cost 0.46774527800379456\n",
      "Training accuracy 91.55\n",
      "Training Cost 0.46770032838717673\n",
      "Training accuracy 91.55624999999999\n",
      "Training Cost 0.46765774538861443\n",
      "Training accuracy 91.55\n",
      "Training Cost 0.4676153557841491\n",
      "Training accuracy 91.57499999999999\n",
      "Training Cost 0.4675712066199685\n",
      "Training accuracy 91.54375\n",
      "Training Cost 0.46752376833880566\n",
      "Training accuracy 91.51875000000001\n",
      "Training Cost 0.4674720409047686\n",
      "Training accuracy 91.51875000000001\n",
      "Training Cost 0.467415571013918\n",
      "Training accuracy 91.53750000000001\n",
      "Training Cost 0.4675712066199685\n",
      "Training accuracy 91.54375\n",
      "Training Cost 0.46752376833880566\n",
      "Training accuracy 91.51875000000001\n",
      "Training Cost 0.4674720409047686\n",
      "Training accuracy 91.51875000000001\n",
      "Training Cost 0.467415571013918\n",
      "Training accuracy 91.53750000000001\n",
      "Training Cost 0.46735439398171613\n",
      "Training accuracy 91.55\n",
      "Training Cost 0.46728891772196857\n",
      "Training accuracy 91.525\n",
      "Training Cost 0.46721977222833844\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.46714765304171335\n",
      "Training accuracy 91.51875000000001\n",
      "Training Cost 0.46707318632914074\n",
      "Training accuracy 91.51875000000001\n",
      "Training Cost 0.46735439398171613\n",
      "Training accuracy 91.55\n",
      "Training Cost 0.46728891772196857\n",
      "Training accuracy 91.525\n",
      "Training Cost 0.46721977222833844\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.46714765304171335\n",
      "Training accuracy 91.51875000000001\n",
      "Training Cost 0.46707318632914074\n",
      "Training accuracy 91.51875000000001\n",
      "Training Cost 0.466996834291987\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.4669188455371276\n",
      "Training accuracy 91.525\n",
      "Training Cost 0.4668392419039524\n",
      "Training accuracy 91.51875000000001\n",
      "Training Cost 0.4667578261651693\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.466996834291987\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.4669188455371276\n",
      "Training accuracy 91.525\n",
      "Training Cost 0.4668392419039524\n",
      "Training accuracy 91.51875000000001\n",
      "Training Cost 0.4667578261651693\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.4666741950103701\n",
      "Training accuracy 91.53125\n",
      "Training Cost 0.46658774622931\n",
      "Training accuracy 91.53125\n",
      "Training Cost 0.4664976749521387\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.4664029598974262\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.46630234787189395\n",
      "Training accuracy 91.51875000000001\n",
      "Training Cost 0.4666741950103701\n",
      "Training accuracy 91.53125\n",
      "Training Cost 0.46658774622931\n",
      "Training accuracy 91.53125\n",
      "Training Cost 0.4664976749521387\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.4664029598974262\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.46630234787189395\n",
      "Training accuracy 91.51875000000001\n",
      "Training Cost 0.46619435611544974\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.4660773306369272\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.4659496226437672\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.4658099544829326\n",
      "Training accuracy 91.53125\n",
      "Training Cost 0.4656579885808041\n",
      "Training accuracy 91.53750000000001\n",
      "Training Cost 0.46619435611544974\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.4660773306369272\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.4659496226437672\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.4658099544829326\n",
      "Training accuracy 91.53125\n",
      "Training Cost 0.4656579885808041\n",
      "Training accuracy 91.53750000000001\n",
      "Training Cost 0.46549493869174435\n",
      "Training accuracy 91.53125\n",
      "Training Cost 0.46532387133276343\n",
      "Training accuracy 91.51875000000001\n",
      "Training Cost 0.46514942099206386\n",
      "Training accuracy 91.525\n",
      "Training Cost 0.4649770025102868\n",
      "Training accuracy 91.53125\n",
      "Training Cost 0.46549493869174435\n",
      "Training accuracy 91.53125\n",
      "Training Cost 0.46532387133276343\n",
      "Training accuracy 91.51875000000001\n",
      "Training Cost 0.46514942099206386\n",
      "Training accuracy 91.525\n",
      "Training Cost 0.4649770025102868\n",
      "Training accuracy 91.53125\n",
      "Training Cost 0.4648117770683097\n",
      "Training accuracy 91.54375\n",
      "Training Cost 0.46465758079083685\n",
      "Training accuracy 91.53750000000001\n",
      "Training Cost 0.46451621712193936\n",
      "Training accuracy 91.53125\n",
      "Training Cost 0.46438752907444786\n",
      "Training accuracy 91.53750000000001\n",
      "Training Cost 0.46427009449567325\n",
      "Training accuracy 91.53750000000001\n",
      "Training Cost 0.4648117770683097\n",
      "Training accuracy 91.54375\n",
      "Training Cost 0.46465758079083685\n",
      "Training accuracy 91.53750000000001\n",
      "Training Cost 0.46451621712193936\n",
      "Training accuracy 91.53125\n",
      "Training Cost 0.46438752907444786\n",
      "Training accuracy 91.53750000000001\n",
      "Training Cost 0.46427009449567325\n",
      "Training accuracy 91.53750000000001\n",
      "Training Cost 0.46416199041027684\n",
      "Training accuracy 91.54375\n",
      "Training Cost 0.46406128349392894\n",
      "Training accuracy 91.55\n",
      "Training Cost 0.46396625065129055\n",
      "Training accuracy 91.54375\n",
      "Training Cost 0.46387545155476445\n",
      "Training accuracy 91.53750000000001\n",
      "Training Cost 0.46416199041027684\n",
      "Training accuracy 91.54375\n",
      "Training Cost 0.46406128349392894\n",
      "Training accuracy 91.55\n",
      "Training Cost 0.46396625065129055\n",
      "Training accuracy 91.54375\n",
      "Training Cost 0.46387545155476445\n",
      "Training accuracy 91.53750000000001\n",
      "Training Cost 0.46378773535821394\n",
      "Training accuracy 91.53750000000001\n",
      "Training Cost 0.4637022161746449\n",
      "Training accuracy 91.53750000000001\n",
      "Training Cost 0.4636182343012231\n",
      "Training accuracy 91.525\n",
      "Training Cost 0.4635353144413855\n",
      "Training accuracy 91.525\n",
      "Training Cost 0.46345312707457303\n",
      "Training accuracy 91.53125\n",
      "Training Cost 0.46378773535821394\n",
      "Training accuracy 91.53750000000001\n",
      "Training Cost 0.4637022161746449\n",
      "Training accuracy 91.53750000000001\n",
      "Training Cost 0.4636182343012231\n",
      "Training accuracy 91.525\n",
      "Training Cost 0.4635353144413855\n",
      "Training accuracy 91.525\n",
      "Training Cost 0.46345312707457303\n",
      "Training accuracy 91.53125\n",
      "Training Cost 0.4633714549390651\n",
      "Training accuracy 91.53125\n",
      "Training Cost 0.4632901643597625\n",
      "Training accuracy 91.525\n",
      "Training Cost 0.46320918020703905\n",
      "Training accuracy 91.55624999999999\n",
      "Training Cost 0.4631284629513629\n",
      "Training accuracy 91.55\n",
      "Training Cost 0.4633714549390651\n",
      "Training accuracy 91.53125\n",
      "Training Cost 0.4632901643597625\n",
      "Training accuracy 91.525\n",
      "Training Cost 0.46320918020703905\n",
      "Training accuracy 91.55624999999999\n",
      "Training Cost 0.4631284629513629\n",
      "Training accuracy 91.55\n",
      "Training Cost 0.4630479864928846\n",
      "Training accuracy 91.56875\n",
      "Training Cost 0.46296771620993793\n",
      "Training accuracy 91.5625\n",
      "Training Cost 0.4628875877508827\n",
      "Training accuracy 91.55\n",
      "Training Cost 0.46280748803904\n",
      "Training accuracy 91.56875\n",
      "Training Cost 0.462727240347818\n",
      "Training accuracy 91.5875\n",
      "Training Cost 0.4630479864928846\n",
      "Training accuracy 91.56875\n",
      "Training Cost 0.46296771620993793\n",
      "Training accuracy 91.5625\n",
      "Training Cost 0.4628875877508827\n",
      "Training accuracy 91.55\n",
      "Training Cost 0.46280748803904\n",
      "Training accuracy 91.56875\n",
      "Training Cost 0.462727240347818\n",
      "Training accuracy 91.5875\n",
      "Training Cost 0.4626465950032406\n",
      "Training accuracy 91.59375\n",
      "Training Cost 0.4625652265521892\n",
      "Training accuracy 91.5875\n",
      "Training Cost 0.4624827376043308\n",
      "Training accuracy 91.58125\n",
      "Training Cost 0.4623986694369043\n",
      "Training accuracy 91.60625\n",
      "Training Cost 0.4626465950032406\n",
      "Training accuracy 91.59375\n",
      "Training Cost 0.4625652265521892\n",
      "Training accuracy 91.5875\n",
      "Training Cost 0.4624827376043308\n",
      "Training accuracy 91.58125\n",
      "Training Cost 0.4623986694369043\n",
      "Training accuracy 91.60625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cost 0.46231251991920524\n",
      "Training accuracy 91.61875\n",
      "Training Cost 0.46222377004144066\n",
      "Training accuracy 91.60625\n",
      "Training Cost 0.4621319207207332\n",
      "Training accuracy 91.60625\n",
      "Training Cost 0.4620365409234935\n",
      "Training accuracy 91.61875\n",
      "Training Cost 0.46231251991920524\n",
      "Training accuracy 91.61875\n",
      "Training Cost 0.46222377004144066\n",
      "Training accuracy 91.60625\n",
      "Training Cost 0.4621319207207332\n",
      "Training accuracy 91.60625\n",
      "Training Cost 0.4620365409234935\n",
      "Training accuracy 91.61875\n",
      "Training Cost 0.46193732594669734\n",
      "Training accuracy 91.6125\n",
      "Training Cost 0.46183416094271945\n",
      "Training accuracy 91.625\n",
      "Training Cost 0.4617271806219986\n",
      "Training accuracy 91.6125\n",
      "Training Cost 0.4616168135067748\n",
      "Training accuracy 91.61875\n",
      "Training Cost 0.4615037972302929\n",
      "Training accuracy 91.63125\n",
      "Training Cost 0.46193732594669734\n",
      "Training accuracy 91.6125\n",
      "Training Cost 0.46183416094271945\n",
      "Training accuracy 91.625\n",
      "Training Cost 0.4617271806219986\n",
      "Training accuracy 91.6125\n",
      "Training Cost 0.4616168135067748\n",
      "Training accuracy 91.61875\n",
      "Training Cost 0.4615037972302929\n",
      "Training accuracy 91.63125\n",
      "Training Cost 0.46138914517291996\n",
      "Training accuracy 91.625\n",
      "Training Cost 0.46127404596501215\n",
      "Training accuracy 91.61875\n",
      "Training Cost 0.4611597157091252\n",
      "Training accuracy 91.63125\n",
      "Training Cost 0.4610472410939928\n",
      "Training accuracy 91.63125\n",
      "Training Cost 0.46138914517291996\n",
      "Training accuracy 91.625\n",
      "Training Cost 0.46127404596501215\n",
      "Training accuracy 91.61875\n",
      "Training Cost 0.4611597157091252\n",
      "Training accuracy 91.63125\n",
      "Training Cost 0.4610472410939928\n",
      "Training accuracy 91.63125\n",
      "Training Cost 0.46093741540705285\n",
      "Training accuracy 91.625\n",
      "Training Cost 0.460830727847253\n",
      "Training accuracy 91.63125\n",
      "Training Cost 0.460727882893048\n",
      "Training accuracy 91.64999999999999\n",
      "Training Cost 0.4606308510564108\n",
      "Training accuracy 91.64999999999999\n",
      "Training Cost 0.4605435219445963\n",
      "Training accuracy 91.6875\n",
      "Training Cost 0.46093741540705285\n",
      "Training accuracy 91.625\n",
      "Training Cost 0.460830727847253\n",
      "Training accuracy 91.63125\n",
      "Training Cost 0.460727882893048\n",
      "Training accuracy 91.64999999999999\n",
      "Training Cost 0.4606308510564108\n",
      "Training accuracy 91.64999999999999\n",
      "Training Cost 0.4605435219445963\n",
      "Training accuracy 91.6875\n",
      "Training Cost 0.46047064540052096\n",
      "Training accuracy 91.65625\n",
      "Training Cost 0.4604152541415774\n",
      "Training accuracy 91.63125\n",
      "Training Cost 0.4603769302594376\n",
      "Training accuracy 91.60625\n",
      "Training Cost 0.4603526442107792\n",
      "Training accuracy 91.625\n",
      "Training Cost 0.46047064540052096\n",
      "Training accuracy 91.65625\n",
      "Training Cost 0.4604152541415774\n",
      "Training accuracy 91.63125\n",
      "Training Cost 0.4603769302594376\n",
      "Training accuracy 91.60625\n",
      "Training Cost 0.4603526442107792\n",
      "Training accuracy 91.625\n",
      "Training Cost 0.46033907384844003\n",
      "Training accuracy 91.64375\n",
      "Training Cost 0.46033432388360873\n",
      "Training accuracy 91.63125\n",
      "Training Cost 0.4603382646470006\n",
      "Training accuracy 91.63125\n",
      "Training Cost 0.46035199008795635\n",
      "Training accuracy 91.625\n",
      "Training Cost 0.46033907384844003\n",
      "Training accuracy 91.64375\n",
      "Training Cost 0.46033432388360873\n",
      "Training accuracy 91.63125\n",
      "Training Cost 0.4603382646470006\n",
      "Training accuracy 91.63125\n",
      "Training Cost 0.46035199008795635\n",
      "Training accuracy 91.625\n",
      "Training Cost 0.4603771243619618\n",
      "Training accuracy 91.63125\n",
      "Training Cost 0.4604153160975747\n",
      "Training accuracy 91.63125\n",
      "Training Cost 0.46046788985557374\n",
      "Training accuracy 91.61875\n",
      "Training Cost 0.46053552860495695\n",
      "Training accuracy 91.63125\n",
      "Training Cost 0.46061796033623853\n",
      "Training accuracy 91.66250000000001\n",
      "Training Cost 0.4603771243619618\n",
      "Training accuracy 91.63125\n",
      "Training Cost 0.4604153160975747\n",
      "Training accuracy 91.63125\n",
      "Training Cost 0.46046788985557374\n",
      "Training accuracy 91.61875\n",
      "Training Cost 0.46053552860495695\n",
      "Training accuracy 91.63125\n",
      "Training Cost 0.46061796033623853\n",
      "Training accuracy 91.66250000000001\n",
      "Training Cost 0.4607137499099696\n",
      "Training accuracy 91.66250000000001\n",
      "Training Cost 0.4608203364836634\n",
      "Training accuracy 91.66875\n",
      "Training Cost 0.4609343708706792\n",
      "Training accuracy 91.68125\n",
      "Training Cost 0.46105225241392594\n",
      "Training accuracy 91.675\n",
      "Training Cost 0.4607137499099696\n",
      "Training accuracy 91.66250000000001\n",
      "Training Cost 0.4608203364836634\n",
      "Training accuracy 91.66875\n",
      "Training Cost 0.4609343708706792\n",
      "Training accuracy 91.68125\n",
      "Training Cost 0.46105225241392594\n",
      "Training accuracy 91.675\n",
      "Training Cost 0.4611706609274175\n",
      "Training accuracy 91.675\n",
      "Training Cost 0.4612869072286686\n",
      "Training accuracy 91.675\n",
      "Training Cost 0.4613990489469804\n",
      "Training accuracy 91.66875\n",
      "Training Cost 0.461505829942866\n",
      "Training accuracy 91.65625\n",
      "Training Cost 0.4611706609274175\n",
      "Training accuracy 91.675\n",
      "Training Cost 0.4612869072286686\n",
      "Training accuracy 91.675\n",
      "Training Cost 0.4613990489469804\n",
      "Training accuracy 91.66875\n",
      "Training Cost 0.461505829942866\n",
      "Training accuracy 91.65625\n",
      "Training Cost 0.4616065381404501\n",
      "Training accuracy 91.65625\n",
      "Training Cost 0.46170085354011575\n",
      "Training accuracy 91.64375\n",
      "Training Cost 0.4617887204625681\n",
      "Training accuracy 91.64999999999999\n",
      "Training Cost 0.4618702520215759\n",
      "Training accuracy 91.64999999999999\n",
      "Training Cost 0.4619456633392059\n",
      "Training accuracy 91.64999999999999\n",
      "Training Cost 0.4616065381404501\n",
      "Training accuracy 91.65625\n",
      "Training Cost 0.46170085354011575\n",
      "Training accuracy 91.64375\n",
      "Training Cost 0.4617887204625681\n",
      "Training accuracy 91.64999999999999\n",
      "Training Cost 0.4618702520215759\n",
      "Training accuracy 91.64999999999999\n",
      "Training Cost 0.4619456633392059\n",
      "Training accuracy 91.64999999999999\n",
      "Training Cost 0.4620152270466803\n",
      "Training accuracy 91.66250000000001\n",
      "Training Cost 0.46207924493938185\n",
      "Training accuracy 91.64999999999999\n",
      "Training Cost 0.4621380308066195\n",
      "Training accuracy 91.64999999999999\n",
      "Training Cost 0.4621919006182129\n",
      "Training Cost 0.4620152270466803\n",
      "Training accuracy 91.66250000000001\n",
      "Training Cost 0.46207924493938185\n",
      "Training accuracy 91.64999999999999\n",
      "Training Cost 0.4621380308066195\n",
      "Training accuracy 91.64999999999999\n",
      "Training Cost 0.4621919006182129\n",
      "Training accuracy 91.64999999999999\n",
      "Training Cost 0.4622411672274301\n",
      "Training accuracy 91.64375\n",
      "Training Cost 0.46228613753113823\n",
      "Training accuracy 91.6375\n",
      "Training Cost 0.4623271106471442\n",
      "Training accuracy 91.64999999999999\n",
      "Training Cost 0.4623643761645993\n",
      "Training accuracy 91.65625\n",
      "Training Cost 0.46239821192514685\n",
      "Training accuracy 91.65625\n",
      "Training accuracy 91.64999999999999\n",
      "Training Cost 0.4622411672274301\n",
      "Training accuracy 91.64375\n",
      "Training Cost 0.46228613753113823\n",
      "Training accuracy 91.6375\n",
      "Training Cost 0.4623271106471442\n",
      "Training accuracy 91.64999999999999\n",
      "Training Cost 0.4623643761645993\n",
      "Training accuracy 91.65625\n",
      "Training Cost 0.46239821192514685\n",
      "Training accuracy 91.65625\n",
      "Training Cost 0.46242888111673736\n",
      "Training accuracy 91.66250000000001\n",
      "Training Cost 0.4624566287169086\n",
      "Training accuracy 91.65625\n",
      "Training Cost 0.4624816775129035\n",
      "Training accuracy 91.64999999999999\n",
      "Training Cost 0.46242888111673736\n",
      "Training accuracy 91.66250000000001\n",
      "Training Cost 0.4624566287169086\n",
      "Training accuracy 91.65625\n",
      "Training Cost 0.4624816775129035\n",
      "Training accuracy 91.64999999999999\n",
      "Training Cost 0.46250422405741515\n",
      "Training accuracy 91.64999999999999\n",
      "Training Cost 0.46252443499617946\n",
      "Training accuracy 91.66250000000001\n",
      "Training Cost 0.46254244423082375\n",
      "Training accuracy 91.65625\n",
      "Training Cost 0.4625583513583709\n",
      "Training accuracy 91.66250000000001\n",
      "Training Cost 0.46257222175649426\n",
      "Training accuracy 91.64999999999999\n",
      "Training Cost 0.46258408856044175\n",
      "Training accuracy 91.64999999999999\n",
      "Training Cost 0.46250422405741515\n",
      "Training accuracy 91.64999999999999\n",
      "Training Cost 0.46252443499617946\n",
      "Training accuracy 91.66250000000001\n",
      "Training Cost 0.46254244423082375\n",
      "Training accuracy 91.65625\n",
      "Training Cost 0.4625583513583709\n",
      "Training accuracy 91.66250000000001\n",
      "Training Cost 0.46257222175649426\n",
      "Training accuracy 91.64999999999999\n",
      "Training Cost 0.46258408856044175\n",
      "Training accuracy 91.64999999999999\n",
      "Training Cost 0.4625939566075715\n",
      "Training accuracy 91.64999999999999\n",
      "Training Cost 0.4626018082213268\n",
      "Training accuracy 91.65625\n",
      "Training Cost 0.462607610491389\n",
      "Training accuracy 91.66875\n",
      "Training Cost 0.4625939566075715\n",
      "Training accuracy 91.64999999999999\n",
      "Training Cost 0.4626018082213268\n",
      "Training accuracy 91.65625\n",
      "Training Cost 0.462607610491389\n",
      "Training accuracy 91.66875\n",
      "Training Cost 0.46261132351260187\n",
      "Training accuracy 91.68125\n",
      "Training Cost 0.46261290890680407\n",
      "Training accuracy 91.68125\n",
      "Training Cost 0.4626123378978633\n",
      "Training accuracy 91.675\n",
      "Training Cost 0.4626095982554038\n",
      "Training accuracy 91.68125\n",
      "Training Cost 0.46260469956156647\n",
      "Training accuracy 91.68125\n",
      "Training Cost 0.462597676462851\n",
      "Training accuracy 91.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cost 0.46261132351260187\n",
      "Training accuracy 91.68125\n",
      "Training Cost 0.46261290890680407\n",
      "Training accuracy 91.68125\n",
      "Training Cost 0.4626123378978633\n",
      "Training accuracy 91.675\n",
      "Training Cost 0.4626095982554038\n",
      "Training accuracy 91.68125\n",
      "Training Cost 0.46260469956156647\n",
      "Training accuracy 91.68125\n",
      "Training Cost 0.462597676462851\n",
      "Training accuracy 91.6875\n",
      "Training Cost 0.4625885898077157\n",
      "Training accuracy 91.66250000000001\n",
      "Training Cost 0.46257752579884753\n",
      "Training accuracy 91.65625\n",
      "Training Cost 0.46256459347223194\n",
      "Training accuracy 91.66250000000001\n",
      "Training Cost 0.4625885898077157\n",
      "Training accuracy 91.66250000000001\n",
      "Training Cost 0.46257752579884753\n",
      "Training accuracy 91.65625\n",
      "Training Cost 0.46256459347223194\n",
      "Training accuracy 91.66250000000001\n",
      "Training Cost 0.4625499209322608\n",
      "Training accuracy 91.66250000000001\n",
      "Training Cost 0.4625336508177346\n",
      "Training accuracy 91.66250000000001\n",
      "Training Cost 0.4625159354559238\n",
      "Training accuracy 91.675\n",
      "Training Cost 0.46249693209765824\n",
      "Training accuracy 91.6875\n",
      "Training Cost 0.46247679853555757\n",
      "Training accuracy 91.6875\n",
      "Training Cost 0.4624556893084378\n",
      "Training accuracy 91.6875\n",
      "Training Cost 0.4625499209322608\n",
      "Training accuracy 91.66250000000001\n",
      "Training Cost 0.4625336508177346\n",
      "Training accuracy 91.66250000000001\n",
      "Training Cost 0.4625159354559238\n",
      "Training accuracy 91.675\n",
      "Training Cost 0.46249693209765824\n",
      "Training accuracy 91.6875\n",
      "Training Cost 0.46247679853555757\n",
      "Training accuracy 91.6875\n",
      "Training Cost 0.4624556893084378\n",
      "Training accuracy 91.6875\n",
      "Training Cost 0.4624337526017756\n",
      "Training accuracy 91.7\n",
      "Training Cost 0.46241112787583066\n",
      "Training accuracy 91.71875\n",
      "Training Cost 0.4624337526017756\n",
      "Training accuracy 91.7\n",
      "Training Cost 0.46241112787583066\n",
      "Training accuracy 91.71875\n",
      "Training Cost 0.46238794419371715\n",
      "Training accuracy 91.73125\n",
      "Training Cost 0.4623643191816812\n",
      "Training accuracy 91.73750000000001\n",
      "Training Cost 0.46234035853097727\n",
      "Training accuracy 91.74375\n",
      "Training Cost 0.4623161559416627\n",
      "Training accuracy 91.74375\n",
      "Training Cost 0.46229179340965165\n",
      "Training accuracy 91.74375\n",
      "Training Cost 0.4622673417660405\n",
      "Training accuracy 91.74375\n",
      "Training Cost 0.46224286138917237\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.46238794419371715\n",
      "Training accuracy 91.73125\n",
      "Training Cost 0.4623643191816812\n",
      "Training accuracy 91.73750000000001\n",
      "Training Cost 0.46234035853097727\n",
      "Training accuracy 91.74375\n",
      "Training Cost 0.4623161559416627\n",
      "Training accuracy 91.74375\n",
      "Training Cost 0.46229179340965165\n",
      "Training accuracy 91.74375\n",
      "Training Cost 0.4622673417660405\n",
      "Training accuracy 91.74375\n",
      "Training Cost 0.46224286138917237\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.4622184030229641\n",
      "Training accuracy 91.75625\n",
      "Training Cost 0.4621940086481281\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.4622184030229641\n",
      "Training accuracy 91.75625\n",
      "Training Cost 0.4621940086481281\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.4621697123651344\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.46214554125850704\n",
      "Training accuracy 91.75625\n",
      "Training Cost 0.4621215162211359\n",
      "Training accuracy 91.7625\n",
      "Training Cost 0.46209765272468617\n",
      "Training accuracy 91.75625\n",
      "Training Cost 0.4620739615280298\n",
      "Training accuracy 91.7625\n",
      "Training Cost 0.4620504493200807\n",
      "Training accuracy 91.75625\n",
      "Training Cost 0.4621697123651344\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.46214554125850704\n",
      "Training accuracy 91.75625\n",
      "Training Cost 0.4621215162211359\n",
      "Training accuracy 91.7625\n",
      "Training Cost 0.46209765272468617\n",
      "Training accuracy 91.75625\n",
      "Training Cost 0.4620739615280298\n",
      "Training accuracy 91.7625\n",
      "Training Cost 0.4620504493200807\n",
      "Training accuracy 91.75625\n",
      "Training Cost 0.4620271192967244\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.46200397167387436\n",
      "Training accuracy 91.74375\n",
      "Training Cost 0.46198100414028676\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.4620271192967244\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.46200397167387436\n",
      "Training accuracy 91.74375\n",
      "Training Cost 0.46198100414028676\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.4619582122547701\n",
      "Training accuracy 91.73750000000001\n",
      "Training Cost 0.46193558979299726\n",
      "Training accuracy 91.73750000000001\n",
      "Training Cost 0.46191312904936643\n",
      "Training accuracy 91.74375\n",
      "Training Cost 0.4618908210993718\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.4618686560277926\n",
      "Training accuracy 91.73750000000001\n",
      "Training Cost 0.4618466231277513\n",
      "Training accuracy 91.73125\n",
      "Training Cost 0.4619582122547701\n",
      "Training accuracy 91.73750000000001\n",
      "Training Cost 0.46193558979299726\n",
      "Training accuracy 91.73750000000001\n",
      "Training Cost 0.46191312904936643\n",
      "Training accuracy 91.74375\n",
      "Training Cost 0.4618908210993718\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.4618686560277926\n",
      "Training accuracy 91.73750000000001\n",
      "Training Cost 0.4618466231277513\n",
      "Training accuracy 91.73125\n",
      "Training Cost 0.46182471107537015\n",
      "Training accuracy 91.725\n",
      "Training Cost 0.4618029080843823\n",
      "Training accuracy 91.725\n",
      "Training Cost 0.46178120204467166\n",
      "Training Cost 0.46182471107537015\n",
      "Training accuracy 91.725\n",
      "Training Cost 0.4618029080843823\n",
      "Training accuracy 91.725\n",
      "Training Cost 0.46178120204467166\n",
      "Training accuracy 91.71875\n",
      "Training Cost 0.4617595806483075\n",
      "Training accuracy 91.73125\n",
      "Training Cost 0.4617380315062361\n",
      "Training accuracy 91.73750000000001\n",
      "Training Cost 0.4617165422583747\n",
      "Training accuracy 91.73125\n",
      "Training Cost 0.46169510067943836\n",
      "Training accuracy 91.74375\n",
      "Training Cost 0.4616736947823948\n",
      "Training accuracy 91.73750000000001\n",
      "Training Cost 0.4616523129210013\n",
      "Training accuracy 91.73125\n",
      "Training accuracy 91.71875\n",
      "Training Cost 0.4617595806483075\n",
      "Training accuracy 91.73125\n",
      "Training Cost 0.4617380315062361\n",
      "Training accuracy 91.73750000000001\n",
      "Training Cost 0.4617165422583747\n",
      "Training accuracy 91.73125\n",
      "Training Cost 0.46169510067943836\n",
      "Training accuracy 91.74375\n",
      "Training Cost 0.4616736947823948\n",
      "Training accuracy 91.73750000000001\n",
      "Training Cost 0.4616523129210013\n",
      "Training accuracy 91.73125\n",
      "Training Cost 0.461630943892424\n",
      "Training accuracy 91.73750000000001\n",
      "Training Cost 0.4616095770404472\n",
      "Training accuracy 91.73125\n",
      "Training Cost 0.461630943892424\n",
      "Training accuracy 91.73750000000001\n",
      "Training Cost 0.4616095770404472\n",
      "Training accuracy 91.73125\n",
      "Training Cost 0.4615882023592889\n",
      "Training accuracy 91.725\n",
      "Training Cost 0.46156681059751603\n",
      "Training accuracy 91.725\n",
      "Training Cost 0.4615453933610226\n",
      "Training accuracy 91.725\n",
      "Training Cost 0.4615239432134956\n",
      "Training accuracy 91.725\n",
      "Training Cost 0.4615024537722973\n",
      "Training accuracy 91.73125\n",
      "Training Cost 0.4614809197971969\n",
      "Training accuracy 91.73125\n",
      "Training Cost 0.46145933726900507\n",
      "Training accuracy 91.73750000000001\n",
      "Training Cost 0.4615882023592889\n",
      "Training accuracy 91.725\n",
      "Training Cost 0.46156681059751603\n",
      "Training accuracy 91.725\n",
      "Training Cost 0.4615453933610226\n",
      "Training accuracy 91.725\n",
      "Training Cost 0.4615239432134956\n",
      "Training accuracy 91.725\n",
      "Training Cost 0.4615024537722973\n",
      "Training accuracy 91.73125\n",
      "Training Cost 0.4614809197971969\n",
      "Training accuracy 91.73125\n",
      "Training Cost 0.46145933726900507\n",
      "Training accuracy 91.73750000000001\n",
      "Training Cost 0.4614377034548516\n",
      "Training accuracy 91.74375\n",
      "Training Cost 0.4614160169566982\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.4614377034548516\n",
      "Training accuracy 91.74375\n",
      "Training Cost 0.4614160169566982\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.46139427773969577\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.4613724871372229\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.461350647829901\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.461328763796577\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.4613068402361871\n",
      "Training accuracy 91.73750000000001\n",
      "Training Cost 0.46128488346053825\n",
      "Training accuracy 91.73750000000001\n",
      "Training Cost 0.4612629007593065\n",
      "Training accuracy 91.74375\n",
      "Training Cost 0.46139427773969577\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.4613724871372229\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.461350647829901\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.461328763796577\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.4613068402361871\n",
      "Training accuracy 91.73750000000001\n",
      "Training Cost 0.46128488346053825\n",
      "Training accuracy 91.73750000000001\n",
      "Training Cost 0.4612629007593065\n",
      "Training accuracy 91.74375\n",
      "Training Cost 0.46124090023988246\n",
      "Training Cost 0.46124090023988246\n",
      "Training accuracy 91.74375\n",
      "Training Cost 0.46121889064600735\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.46119688116032764\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.4611748811969667\n",
      "Training accuracy 91.75625\n",
      "Training Cost 0.4611529001908723\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.4611309473909849\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.46110903166416123\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.46108716131626043\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.46106534393592336\n",
      "Training accuracy 91.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 91.74375\n",
      "Training Cost 0.46121889064600735\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.46119688116032764\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.4611748811969667\n",
      "Training accuracy 91.75625\n",
      "Training Cost 0.4611529001908723\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.4611309473909849\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.46110903166416123\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.46108716131626043\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.46106534393592336\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.461043586265391\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.46102189410133426\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.461000272227195\n",
      "Training accuracy 91.75625\n",
      "Training Cost 0.4609787243770891\n",
      "Training accuracy 91.78125\n",
      "Training Cost 0.4609572532299866\n",
      "Training accuracy 91.78125\n",
      "Training Cost 0.46093586043174695\n",
      "Training accuracy 91.7875\n",
      "Training Cost 0.4609145466417006\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.46089331159986\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.4608721542105185\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.461043586265391\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.46102189410133426\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.461000272227195\n",
      "Training accuracy 91.75625\n",
      "Training Cost 0.4609787243770891\n",
      "Training accuracy 91.78125\n",
      "Training Cost 0.4609572532299866\n",
      "Training accuracy 91.78125\n",
      "Training Cost 0.46093586043174695\n",
      "Training accuracy 91.7875\n",
      "Training Cost 0.4609145466417006\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.46089331159986\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.4608721542105185\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.460851072637922\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.46083006440985846\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.4608091265253347\n",
      "Training accuracy 91.80625\n",
      "Training Cost 0.46078825556296965\n",
      "Training accuracy 91.81875000000001\n",
      "Training Cost 0.46076744778725837\n",
      "Training accuracy 91.825\n",
      "Training Cost 0.4607466992504254\n",
      "Training accuracy 91.80625\n",
      "Training Cost 0.46072600588814017\n",
      "Training accuracy 91.825\n",
      "Training Cost 0.46070536360788683\n",
      "Training accuracy 91.81875000000001\n",
      "Training Cost 0.46068476836925193\n",
      "Training accuracy 91.8125\n",
      "Training Cost 0.460851072637922\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.46083006440985846\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.4608091265253347\n",
      "Training accuracy 91.80625\n",
      "Training Cost 0.46078825556296965\n",
      "Training accuracy 91.81875000000001\n",
      "Training Cost 0.46076744778725837\n",
      "Training accuracy 91.825\n",
      "Training Cost 0.4607466992504254\n",
      "Training accuracy 91.80625\n",
      "Training Cost 0.46072600588814017\n",
      "Training accuracy 91.825\n",
      "Training Cost 0.46070536360788683\n",
      "Training accuracy 91.81875000000001\n",
      "Training Cost 0.46068476836925193\n",
      "Training accuracy 91.8125\n",
      "Training Cost 0.46066421625578935\n",
      "Training accuracy 91.8125\n",
      "Training Cost 0.46064370353845974\n",
      "Training accuracy 91.80625\n",
      "Training Cost 0.46062322673089706\n",
      "Training accuracy 91.80625\n",
      "Training Cost 0.4606027826369519\n",
      "Training accuracy 91.8125\n",
      "Training Cost 0.4605823683911001\n",
      "Training accuracy 91.80625\n",
      "Training Cost 0.4605619814923826\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.4605416198326004\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.4605212817194893\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.46050096589561135\n",
      "Training accuracy 91.79375\n",
      "Training Cost 0.46066421625578935\n",
      "Training accuracy 91.8125\n",
      "Training Cost 0.46064370353845974\n",
      "Training accuracy 91.80625\n",
      "Training Cost 0.46062322673089706\n",
      "Training accuracy 91.80625\n",
      "Training Cost 0.4606027826369519\n",
      "Training accuracy 91.8125\n",
      "Training Cost 0.4605823683911001\n",
      "Training accuracy 91.80625\n",
      "Training Cost 0.4605619814923826\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.4605416198326004\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.4605212817194893\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.46050096589561135\n",
      "Training accuracy 91.79375\n",
      "Training Cost 0.4604806715536755\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.460460398348996\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.46044014640979736\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.46041991634609036\n",
      "Training accuracy 91.79375\n",
      "Training Cost 0.460399709257892\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.4603795267436371\n",
      "Training accuracy 91.80625\n",
      "Training Cost 0.4603593709097358\n",
      "Training accuracy 91.825\n",
      "Training Cost 0.4603392443823665\n",
      "Training accuracy 91.81875000000001\n",
      "Training Cost 0.4603191503227586\n",
      "Training accuracy 91.8125\n",
      "Training Cost 0.4604806715536755\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.460460398348996\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.46044014640979736\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.46041991634609036\n",
      "Training accuracy 91.79375\n",
      "Training Cost 0.460399709257892\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.4603795267436371\n",
      "Training accuracy 91.80625\n",
      "Training Cost 0.4603593709097358\n",
      "Training accuracy 91.825\n",
      "Training Cost 0.4603392443823665\n",
      "Training accuracy 91.81875000000001\n",
      "Training Cost 0.4603191503227586\n",
      "Training accuracy 91.8125\n",
      "Training Cost 0.4602990924473794\n",
      "Training accuracy 91.825\n",
      "Training Cost 0.46027907505461935\n",
      "Training accuracy 91.83125\n",
      "Training Cost 0.46025910305969514\n",
      "Training accuracy 91.8375\n",
      "Training Cost 0.46023918203958697\n",
      "Training accuracy 91.84375\n",
      "Training Cost 0.46021931828982965\n",
      "Training accuracy 91.84375\n",
      "Training Cost 0.46019951889488064\n",
      "Training accuracy 91.84375\n",
      "Training Cost 0.4601797918135571\n",
      "Training accuracy 91.8375\n",
      "Training Cost 0.4602990924473794\n",
      "Training accuracy 91.825\n",
      "Training Cost 0.46027907505461935\n",
      "Training accuracy 91.83125\n",
      "Training Cost 0.46025910305969514\n",
      "Training accuracy 91.8375\n",
      "Training Cost 0.46023918203958697\n",
      "Training accuracy 91.84375\n",
      "Training Cost 0.46021931828982965\n",
      "Training accuracy 91.84375\n",
      "Training Cost 0.46019951889488064\n",
      "Training accuracy 91.84375\n",
      "Training Cost 0.4601797918135571\n",
      "Training accuracy 91.8375\n",
      "Training Cost 0.4601601459806488\n",
      "Training accuracy 91.8375\n",
      "Training Cost 0.4601405914252697\n",
      "Training accuracy 91.8375\n",
      "Training Cost 0.4601211394057948\n",
      "Training accuracy 91.8375\n",
      "Training Cost 0.4601018025603764\n",
      "Training accuracy 91.83125\n",
      "Training Cost 0.46008259507105315\n",
      "Training accuracy 91.8375\n",
      "Training Cost 0.4600635328384417\n",
      "Training accuracy 91.8375\n",
      "Training Cost 0.46004463366297693\n",
      "Training accuracy 91.8375\n",
      "Training Cost 0.4600259174277655\n",
      "Training accuracy 91.83125\n",
      "Training Cost 0.4600074062774326\n",
      "Training accuracy 91.825\n",
      "Training Cost 0.4601601459806488\n",
      "Training accuracy 91.8375\n",
      "Training Cost 0.4601405914252697\n",
      "Training accuracy 91.8375\n",
      "Training Cost 0.4601211394057948\n",
      "Training accuracy 91.8375\n",
      "Training Cost 0.4601018025603764\n",
      "Training accuracy 91.83125\n",
      "Training Cost 0.46008259507105315\n",
      "Training accuracy 91.8375\n",
      "Training Cost 0.4600635328384417\n",
      "Training accuracy 91.8375\n",
      "Training Cost 0.46004463366297693\n",
      "Training accuracy 91.8375\n",
      "Training Cost 0.4600259174277655\n",
      "Training accuracy 91.83125\n",
      "Training Cost 0.4600074062774326\n",
      "Training accuracy 91.825\n",
      "Training Cost 0.4599891247869966\n",
      "Training accuracy 91.81875000000001\n",
      "Training Cost 0.4599711001149332\n",
      "Training accuracy 91.81875000000001\n",
      "Training Cost 0.45995336213525645\n",
      "Training accuracy 91.80625\n",
      "Training Cost 0.4599359435447678\n",
      "Training accuracy 91.8125\n",
      "Training Cost 0.45991887994355946\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.45990220988941016\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.4598859749297079\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.4598702196177796\n",
      "Training accuracy 91.80625\n",
      "Training Cost 0.4598549915237288\n",
      "Training accuracy 91.8125\n",
      "Training Cost 0.4599891247869966\n",
      "Training accuracy 91.81875000000001\n",
      "Training Cost 0.4599711001149332\n",
      "Training accuracy 91.81875000000001\n",
      "Training Cost 0.45995336213525645\n",
      "Training accuracy 91.80625\n",
      "Training Cost 0.4599359435447678\n",
      "Training accuracy 91.8125\n",
      "Training Cost 0.45991887994355946\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.45990220988941016\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.4598859749297079\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.4598702196177796\n",
      "Training accuracy 91.80625\n",
      "Training Cost 0.4598549915237288\n",
      "Training accuracy 91.8125\n",
      "Training Cost 0.4598403412527393\n",
      "Training accuracy 91.80625\n",
      "Training Cost 0.45982632248595634\n",
      "Training accuracy 91.81875000000001\n",
      "Training Cost 0.4598129920601672\n",
      "Training accuracy 91.81875000000001\n",
      "Training Cost 0.4598004101022705\n",
      "Training accuracy 91.83125\n",
      "Training Cost 0.4597886402326459\n",
      "Training accuracy 91.825\n",
      "Training Cost 0.4597777498477364\n",
      "Training accuracy 91.83125\n",
      "Training Cost 0.4597678104859855\n",
      "Training accuracy 91.825\n",
      "Training Cost 0.4597588982720393\n",
      "Training accuracy 91.825\n",
      "Training Cost 0.4597510944204773\n",
      "Training accuracy 91.825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cost 0.4598403412527393\n",
      "Training accuracy 91.80625\n",
      "Training Cost 0.45982632248595634\n",
      "Training accuracy 91.81875000000001\n",
      "Training Cost 0.4598129920601672\n",
      "Training accuracy 91.81875000000001\n",
      "Training Cost 0.4598004101022705\n",
      "Training accuracy 91.83125\n",
      "Training Cost 0.4597886402326459\n",
      "Training accuracy 91.825\n",
      "Training Cost 0.4597777498477364\n",
      "Training accuracy 91.83125\n",
      "Training Cost 0.4597678104859855\n",
      "Training accuracy 91.825\n",
      "Training Cost 0.4597588982720393\n",
      "Training accuracy 91.825\n",
      "Training Cost 0.4597510944204773\n",
      "Training accuracy 91.825\n",
      "Training Cost 0.45974448575986915\n",
      "Training accuracy 91.825\n",
      "Training Cost 0.4597391652063109\n",
      "Training accuracy 91.83125\n",
      "Training Cost 0.4597352320650244\n",
      "Training accuracy 91.81875000000001\n",
      "Training Cost 0.45973279195549344\n",
      "Training accuracy 91.8125\n",
      "Training Cost 0.4597319560156261\n",
      "Training accuracy 91.80625\n",
      "Training Cost 0.45973283879978705\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.4597355538636108\n",
      "Training accuracy 91.7875\n",
      "Training Cost 0.4597402052739165\n",
      "Training accuracy 91.79375\n",
      "Training Cost 0.45974687190358\n",
      "Training accuracy 91.7875\n",
      "Training Cost 0.45974448575986915\n",
      "Training accuracy 91.825\n",
      "Training Cost 0.4597391652063109\n",
      "Training accuracy 91.83125\n",
      "Training Cost 0.4597352320650244\n",
      "Training accuracy 91.81875000000001\n",
      "Training Cost 0.45973279195549344\n",
      "Training accuracy 91.8125\n",
      "Training Cost 0.4597319560156261\n",
      "Training accuracy 91.80625\n",
      "Training Cost 0.45973283879978705\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.4597355538636108\n",
      "Training accuracy 91.7875\n",
      "Training Cost 0.4597402052739165\n",
      "Training accuracy 91.79375\n",
      "Training Cost 0.45974687190358\n",
      "Training accuracy 91.7875\n",
      "Training Cost 0.4597555787962448\n",
      "Training accuracy 91.7875\n",
      "Training Cost 0.4597662449649327\n",
      "Training accuracy 91.79375\n",
      "Training Cost 0.4597785873861302\n",
      "Training accuracy 91.7875\n",
      "Training Cost 0.45979194194690143\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.4598049246854852\n",
      "Training accuracy 91.79375\n",
      "Training Cost 0.4598147874260233\n",
      "Training accuracy 91.78125\n",
      "Training Cost 0.459816227001675\n",
      "Training accuracy 91.77499999999999\n",
      "Training Cost 0.45979951578965883\n",
      "Training accuracy 91.7875\n",
      "validation accuracy\n",
      "91.05\n",
      "Training Cost 0.4597555787962448\n",
      "Training accuracy 91.7875\n",
      "Training Cost 0.4597662449649327\n",
      "Training accuracy 91.79375\n",
      "Training Cost 0.4597785873861302\n",
      "Training accuracy 91.7875\n",
      "Training Cost 0.45979194194690143\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.4598049246854852\n",
      "Training accuracy 91.79375\n",
      "Training Cost 0.4598147874260233\n",
      "Training accuracy 91.78125\n",
      "Training Cost 0.459816227001675\n",
      "Training accuracy 91.77499999999999\n",
      "Training Cost 0.45979951578965883\n",
      "Training accuracy 91.7875\n",
      "validation accuracy\n",
      "91.05\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for iteration in range(int(len(trainData)/batch)):\n",
    "        a0,z1,a1,z2,a2=forwFunc(trainData[iteration*batch:(iteration+1)*batch,:])\n",
    "        y=a2\n",
    "        labelBatch=trainLabel[iteration*batch:(iteration+1)*batch,:]\n",
    "        del2=(y-labelBatch)\n",
    "        del1=np.dot(del2,w2.T)*(1 - a1**2)\n",
    "        dcdw2=np.dot(a1.T,del2)\n",
    "        dcdw1=np.dot(a0.T,del1)\n",
    "        dcdb1=np.sum(del1,axis=0)\n",
    "        dcdb2=np.sum(del2,axis=0)\n",
    "        w1=w1-alpha*dcdw1\n",
    "        w2=w2-alpha*dcdw2\n",
    "        b2=b2-alpha*dcdb2\n",
    "        b1=b1-alpha*dcdb1\n",
    "    a0,z1,a1,z2,a2=forwFunc(trainData)\n",
    "    print(\"Training Cost\",(np.sum(costFunc(a2,trainLabel)))/16000.0)\n",
    "    print(\"Training accuracy\",Acc(a2,trainLabel,len(trainLabel)))\n",
    "    \n",
    "va0,vz1,va1,vz2,va2=forwFunc(validData)\n",
    "vOutput=va2\n",
    "accuracy = Acc(vOutput,validLabel,2000.0)\n",
    "print(\"validation accuracy\")\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
